{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d0853d",
   "metadata": {},
   "source": [
    "# Citi Bike Trip Data Pipeline 🚲\n",
    "End-to-end data pipeline for:\n",
    "- Downloading 12 months of Citi Bike data\n",
    "- Extracting and filtering for Top 3 stations\n",
    "- Cleaning and engineering features\n",
    "- Uploading to Hopsworks Feature Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47958175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\bike_Taxi\\bike_Taxienv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Hopsworks\n",
    "import hopsworks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed855588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable Constants\n",
    "TEMP_FOLDER = \"tripdata_temp\"\n",
    "OUTPUT_FILE = \"top3_stations_output.csv\"\n",
    "CHUNK_SIZE = 500_000\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"ride_id\", \"rideable_type\", \"started_at\", \"ended_at\",\n",
    "    \"start_station_name\", \"start_station_id\",\n",
    "    \"end_station_name\", \"end_station_id\",\n",
    "    \"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\",\n",
    "    \"member_casual\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526244f",
   "metadata": {},
   "source": [
    "## ⏳ Get Last 12 Months in US/Eastern Timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088c3fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['202504',\n",
       " '202503',\n",
       " '202502',\n",
       " '202501',\n",
       " '202412',\n",
       " '202411',\n",
       " '202410',\n",
       " '202409',\n",
       " '202408',\n",
       " '202407',\n",
       " '202406',\n",
       " '202405']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_last_12_months_est():\n",
    "    eastern = pytz.timezone(\"US/Eastern\")\n",
    "    now_est = datetime.now(eastern)\n",
    "    return [(now_est - relativedelta(months=i + 1)).strftime('%Y%m') for i in range(12)]\n",
    "\n",
    "months = get_last_12_months_est()\n",
    "months\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4e3eb",
   "metadata": {},
   "source": [
    "## 🌐 Download Citi Bike ZIPs\n",
    "Tries both `.zip` and `.csv.zip` formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743f869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip_to_memory(ym):\n",
    "    base_url = \"https://s3.amazonaws.com/tripdata/\"\n",
    "    filenames = [f\"{ym}-citibike-tripdata.zip\", f\"{ym}-citibike-tripdata.csv.zip\"]\n",
    "    for fname in filenames:\n",
    "        url = base_url + fname\n",
    "        try:\n",
    "            print(f\"🌐 Trying: {url}\")\n",
    "            r = requests.get(url, timeout=20)\n",
    "            if r.status_code == 200:\n",
    "                print(f\"✅ Downloaded: {fname}\")\n",
    "                return BytesIO(r.content)\n",
    "            else:\n",
    "                print(f\"❌ Not found: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error downloading {url}: {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6f0a7",
   "metadata": {},
   "source": [
    "## 🗂️ Extract CSVs (including nested ZIPs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8400a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_csvs(zip_bytes_io, extract_to):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_bytes_io) as zf:\n",
    "            for member in zf.namelist():\n",
    "                if member.endswith('.zip'):\n",
    "                    nested_zip_data = zf.read(member)\n",
    "                    with zipfile.ZipFile(BytesIO(nested_zip_data)) as nested_zf:\n",
    "                        for nested_member in nested_zf.namelist():\n",
    "                            if nested_member.endswith('.csv'):\n",
    "                                print(f\"📦 Extracting nested CSV: {nested_member}\")\n",
    "                                nested_zf.extract(nested_member, extract_to)\n",
    "                elif member.endswith('.csv'):\n",
    "                    print(f\"📁 Extracting CSV: {member}\")\n",
    "                    zf.extract(member, extract_to)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error extracting zip: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c17c6d",
   "metadata": {},
   "source": [
    "## 📁 Flatten CSV Paths from Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b026122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_csvs_folder(root_folder):\n",
    "    flat_files = []\n",
    "    for root, _, files in os.walk(root_folder):\n",
    "        for fname in files:\n",
    "            if fname.endswith(\".csv\"):\n",
    "                full_path = os.path.join(root, fname)\n",
    "                flat_files.append(full_path)\n",
    "    return flat_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e75e6a",
   "metadata": {},
   "source": [
    "## 📊 Identify Top 3 Most Frequent Start Stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f929007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top3_station_names(filepaths):\n",
    "    freq = Counter()\n",
    "    for path in filepaths:\n",
    "        try:\n",
    "            for chunk in pd.read_csv(path, usecols=[\"start_station_name\"], dtype={\"start_station_name\": str}, chunksize=CHUNK_SIZE):\n",
    "                chunk = chunk.dropna(subset=[\"start_station_name\"])\n",
    "                freq.update(chunk[\"start_station_name\"])\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {path}: {e}\")\n",
    "    top3 = [name for name, _ in freq.most_common(3)]\n",
    "    return top3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241077aa",
   "metadata": {},
   "source": [
    "## 📝 Write Filtered Rows (Only Top 3 Stations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e0107bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_top3_data(filepaths, top3, output=OUTPUT_FILE):\n",
    "    first_write = True\n",
    "    for path in filepaths:\n",
    "        try:\n",
    "            for chunk in pd.read_csv(path, usecols=TARGET_COLS, chunksize=CHUNK_SIZE, low_memory=False):\n",
    "                chunk = chunk.dropna(subset=[\"start_station_name\"])\n",
    "                filtered = chunk[chunk[\"start_station_name\"].isin(top3)]\n",
    "                if not filtered.empty:\n",
    "                    filtered.to_csv(output, index=False, mode='a' if not first_write else 'w', header=first_write)\n",
    "                    print(f\"✅ Written {len(filtered)} rows from {path}\")\n",
    "                    first_write = False\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02463a77",
   "metadata": {},
   "source": [
    "## 🧼 Clean Data and Engineer Features\n",
    "- Handle nulls\n",
    "- Parse timestamps\n",
    "- Add ride_duration, day_of_week, hour_of_day, month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189947d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_engineer_features(file_path):\n",
    "    print(\"🧼 Cleaning and engineering features...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    df = df.dropna(subset=['started_at', 'ended_at'])\n",
    "\n",
    "    critical_cols = ['ride_id', 'rideable_type', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'member_casual']\n",
    "    df = df.dropna(subset=critical_cols)\n",
    "\n",
    "    df['start_station_name'] = df['start_station_name'].fillna('Unknown')\n",
    "    df['end_station_name'] = df['end_station_name'].fillna('Unknown')\n",
    "    df['start_station_id'] = df['start_station_id'].fillna('-1').astype(str)\n",
    "    df['end_station_id'] = df['end_station_id'].fillna('-1').astype(str)\n",
    "\n",
    "    df['rideable_type'] = df['rideable_type'].astype('category')\n",
    "    df['member_casual'] = df['member_casual'].astype('category')\n",
    "\n",
    "    df['ride_duration_mins'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "    df = df[df['ride_duration_mins'] > 0]\n",
    "\n",
    "    df['day_of_week'] = df['started_at'].dt.day_name()\n",
    "    df['hour_of_day'] = df['started_at'].dt.hour.astype('int32')\n",
    "    df['month'] = df['started_at'].dt.month.astype('int32')\n",
    "\n",
    "    print(f\"✅ Cleaned dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ef209",
   "metadata": {},
   "source": [
    "## 🔐 Connect and Push to Hopsworks Feature Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8027cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_hopsworks():\n",
    "    print(\"🔐 Connecting to Hopsworks...\")\n",
    "    load_dotenv()\n",
    "    project = hopsworks.login(\n",
    "        project=os.getenv(\"HOPSWORKS_PROJECT\"),\n",
    "        api_key_value=os.getenv(\"HOPSWORKS_API_KEY\")\n",
    "    )\n",
    "    return project.get_feature_store()\n",
    "\n",
    "def push_to_feature_store(df, fs):\n",
    "    print(\"🚀 Pushing to Hopsworks Feature Store...\")\n",
    "\n",
    "    try:\n",
    "        # Delete the entire feature group if it exists\n",
    "        fg = fs.get_feature_group(\"citi_bike_trips\", version=1)\n",
    "        fg.delete()\n",
    "        print(\"🗑️ Existing feature group deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ℹ️ Feature group does not exist or could not be deleted: {e}\")\n",
    "\n",
    "    # Recreate feature group\n",
    "    fg = fs.create_feature_group(\n",
    "        name=\"citi_bike_trips\",\n",
    "        version=1,\n",
    "        description=\"Citi Bike data from top 3 stations in last 12 months\",\n",
    "        primary_key=[\"ride_id\"],\n",
    "        event_time=\"started_at\"\n",
    "    )\n",
    "\n",
    "    fg.insert(df, write_options={\"wait_for_job\": True})\n",
    "    print(\"✅ Feature group created and data inserted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cfd2d",
   "metadata": {},
   "source": [
    "## ⚙️ Run the Full ETL Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9cf5ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting download + extraction...\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202504-citibike-tripdata.zip\n",
      "✅ Downloaded: 202504-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202504-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202504-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202504-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202504-citibike-tripdata_4.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202503-citibike-tripdata.zip\n",
      "❌ Not found: https://s3.amazonaws.com/tripdata/202503-citibike-tripdata.zip\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202503-citibike-tripdata.csv.zip\n",
      "✅ Downloaded: 202503-citibike-tripdata.csv.zip\n",
      "📁 Extracting CSV: 202503-citibike-tripdata.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202502-citibike-tripdata.zip\n",
      "✅ Downloaded: 202502-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202502-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202502-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202502-citibike-tripdata_1.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202501-citibike-tripdata.zip\n",
      "✅ Downloaded: 202501-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202501-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202501-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202501-citibike-tripdata_2.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202412-citibike-tripdata.zip\n",
      "✅ Downloaded: 202412-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202412-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202412-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202412-citibike-tripdata_2.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202411-citibike-tripdata.zip\n",
      "✅ Downloaded: 202411-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202411-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202411-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202411-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202411-citibike-tripdata_4.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202410-citibike-tripdata.zip\n",
      "✅ Downloaded: 202410-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_6.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_5.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_3.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202409-citibike-tripdata.zip\n",
      "✅ Downloaded: 202409-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_5.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_5.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202408-citibike-tripdata.zip\n",
      "✅ Downloaded: 202408-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_5.csv\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_4.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202407-citibike-tripdata.zip\n",
      "✅ Downloaded: 202407-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_5.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202406-citibike-tripdata.zip\n",
      "✅ Downloaded: 202406-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_5.csv\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_2.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202405-citibike-tripdata.zip\n",
      "✅ Downloaded: 202405-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_5.csv\n",
      "🔍 Counting top 3 stations...\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_1.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_2.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_3.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_4.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_5.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "🏆 Top 3 Stations: ['W 21 St & 6 Ave', 'University Pl & E 14 St', 'Lafayette St & E 8 St']\n",
      "📤 Writing filtered data...\n",
      "✅ Written 4269 rows from tripdata_temp\\202405-citibike-tripdata_1.csv\n",
      "✅ Written 4148 rows from tripdata_temp\\202405-citibike-tripdata_1.csv\n",
      "✅ Written 6092 rows from tripdata_temp\\202405-citibike-tripdata_2.csv\n",
      "✅ Written 6035 rows from tripdata_temp\\202405-citibike-tripdata_2.csv\n",
      "✅ Written 3887 rows from tripdata_temp\\202405-citibike-tripdata_3.csv\n",
      "✅ Written 3906 rows from tripdata_temp\\202405-citibike-tripdata_3.csv\n",
      "✅ Written 5128 rows from tripdata_temp\\202405-citibike-tripdata_4.csv\n",
      "✅ Written 4831 rows from tripdata_temp\\202405-citibike-tripdata_4.csv\n",
      "✅ Written 1726 rows from tripdata_temp\\202405-citibike-tripdata_5.csv\n",
      "✅ Written 5017 rows from tripdata_temp\\202406-citibike-tripdata_1.csv\n",
      "✅ Written 4133 rows from tripdata_temp\\202406-citibike-tripdata_1.csv\n",
      "✅ Written 4413 rows from tripdata_temp\\202406-citibike-tripdata_2.csv\n",
      "✅ Written 5777 rows from tripdata_temp\\202406-citibike-tripdata_2.csv\n",
      "✅ Written 5494 rows from tripdata_temp\\202406-citibike-tripdata_3.csv\n",
      "✅ Written 4376 rows from tripdata_temp\\202406-citibike-tripdata_3.csv\n",
      "✅ Written 5421 rows from tripdata_temp\\202406-citibike-tripdata_4.csv\n",
      "✅ Written 4826 rows from tripdata_temp\\202406-citibike-tripdata_4.csv\n",
      "✅ Written 4404 rows from tripdata_temp\\202406-citibike-tripdata_5.csv\n",
      "✅ Written 3119 rows from tripdata_temp\\202406-citibike-tripdata_5.csv\n",
      "✅ Written 4602 rows from tripdata_temp\\202407-citibike-tripdata_1.csv\n",
      "✅ Written 4556 rows from tripdata_temp\\202407-citibike-tripdata_1.csv\n",
      "✅ Written 4116 rows from tripdata_temp\\202407-citibike-tripdata_2.csv\n",
      "✅ Written 5066 rows from tripdata_temp\\202407-citibike-tripdata_2.csv\n",
      "✅ Written 2152 rows from tripdata_temp\\202407-citibike-tripdata_3.csv\n",
      "✅ Written 4749 rows from tripdata_temp\\202407-citibike-tripdata_3.csv\n",
      "✅ Written 4638 rows from tripdata_temp\\202407-citibike-tripdata_4.csv\n",
      "✅ Written 5166 rows from tripdata_temp\\202407-citibike-tripdata_4.csv\n",
      "✅ Written 6684 rows from tripdata_temp\\202407-citibike-tripdata_5.csv\n",
      "✅ Written 2709 rows from tripdata_temp\\202407-citibike-tripdata_5.csv\n",
      "✅ Written 4395 rows from tripdata_temp\\202408-citibike-tripdata_1.csv\n",
      "✅ Written 5096 rows from tripdata_temp\\202408-citibike-tripdata_1.csv\n",
      "✅ Written 5047 rows from tripdata_temp\\202408-citibike-tripdata_2.csv\n",
      "✅ Written 5211 rows from tripdata_temp\\202408-citibike-tripdata_2.csv\n",
      "✅ Written 5087 rows from tripdata_temp\\202408-citibike-tripdata_3.csv\n",
      "✅ Written 4756 rows from tripdata_temp\\202408-citibike-tripdata_3.csv\n",
      "✅ Written 4187 rows from tripdata_temp\\202408-citibike-tripdata_4.csv\n",
      "✅ Written 5129 rows from tripdata_temp\\202408-citibike-tripdata_4.csv\n",
      "✅ Written 4760 rows from tripdata_temp\\202408-citibike-tripdata_5.csv\n",
      "✅ Written 3318 rows from tripdata_temp\\202409-citibike-tripdata_1.csv\n",
      "✅ Written 4382 rows from tripdata_temp\\202409-citibike-tripdata_1.csv\n",
      "✅ Written 3458 rows from tripdata_temp\\202409-citibike-tripdata_2.csv\n",
      "✅ Written 2715 rows from tripdata_temp\\202409-citibike-tripdata_2.csv\n",
      "✅ Written 4848 rows from tripdata_temp\\202409-citibike-tripdata_3.csv\n",
      "✅ Written 4514 rows from tripdata_temp\\202409-citibike-tripdata_3.csv\n",
      "✅ Written 2843 rows from tripdata_temp\\202409-citibike-tripdata_4.csv\n",
      "✅ Written 6104 rows from tripdata_temp\\202409-citibike-tripdata_4.csv\n",
      "✅ Written 5337 rows from tripdata_temp\\202409-citibike-tripdata_5.csv\n",
      "✅ Written 7643 rows from tripdata_temp\\202409-citibike-tripdata_5.csv\n",
      "✅ Written 4622 rows from tripdata_temp\\202411-citibike-tripdata_1.csv\n",
      "✅ Written 4416 rows from tripdata_temp\\202411-citibike-tripdata_1.csv\n",
      "✅ Written 5510 rows from tripdata_temp\\202411-citibike-tripdata_2.csv\n",
      "✅ Written 6165 rows from tripdata_temp\\202411-citibike-tripdata_2.csv\n",
      "✅ Written 5355 rows from tripdata_temp\\202411-citibike-tripdata_3.csv\n",
      "✅ Written 4250 rows from tripdata_temp\\202411-citibike-tripdata_3.csv\n",
      "✅ Written 5364 rows from tripdata_temp\\202411-citibike-tripdata_4.csv\n",
      "✅ Written 2489 rows from tripdata_temp\\202411-citibike-tripdata_4.csv\n",
      "✅ Written 4474 rows from tripdata_temp\\202412-citibike-tripdata_1.csv\n",
      "✅ Written 5606 rows from tripdata_temp\\202412-citibike-tripdata_1.csv\n",
      "✅ Written 7089 rows from tripdata_temp\\202412-citibike-tripdata_2.csv\n",
      "✅ Written 3864 rows from tripdata_temp\\202412-citibike-tripdata_2.csv\n",
      "✅ Written 3053 rows from tripdata_temp\\202412-citibike-tripdata_3.csv\n",
      "✅ Written 4981 rows from tripdata_temp\\202501-citibike-tripdata_1.csv\n",
      "✅ Written 4953 rows from tripdata_temp\\202501-citibike-tripdata_1.csv\n",
      "✅ Written 6306 rows from tripdata_temp\\202501-citibike-tripdata_2.csv\n",
      "✅ Written 5259 rows from tripdata_temp\\202501-citibike-tripdata_2.csv\n",
      "✅ Written 1121 rows from tripdata_temp\\202501-citibike-tripdata_3.csv\n",
      "✅ Written 6623 rows from tripdata_temp\\202502-citibike-tripdata_1.csv\n",
      "✅ Written 4569 rows from tripdata_temp\\202502-citibike-tripdata_1.csv\n",
      "✅ Written 5672 rows from tripdata_temp\\202502-citibike-tripdata_2.csv\n",
      "✅ Written 4767 rows from tripdata_temp\\202502-citibike-tripdata_2.csv\n",
      "✅ Written 340 rows from tripdata_temp\\202502-citibike-tripdata_3.csv\n",
      "✅ Written 5138 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 5613 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 3726 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 4735 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 5927 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 4848 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 1620 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 4910 rows from tripdata_temp\\202504-citibike-tripdata_1.csv\n",
      "✅ Written 4890 rows from tripdata_temp\\202504-citibike-tripdata_1.csv\n",
      "✅ Written 4610 rows from tripdata_temp\\202504-citibike-tripdata_2.csv\n",
      "✅ Written 3922 rows from tripdata_temp\\202504-citibike-tripdata_2.csv\n",
      "✅ Written 6138 rows from tripdata_temp\\202504-citibike-tripdata_3.csv\n",
      "✅ Written 4733 rows from tripdata_temp\\202504-citibike-tripdata_3.csv\n",
      "✅ Written 5533 rows from tripdata_temp\\202504-citibike-tripdata_4.csv\n",
      "✅ Written 2275 rows from tripdata_temp\\202504-citibike-tripdata_4.csv\n",
      "✅ Written 4605 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_1.csv\n",
      "✅ Written 4566 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_1.csv\n",
      "✅ Written 5732 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_2.csv\n",
      "✅ Written 5151 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_2.csv\n",
      "✅ Written 3776 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_3.csv\n",
      "✅ Written 4481 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_3.csv\n",
      "✅ Written 5387 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_4.csv\n",
      "✅ Written 5707 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_4.csv\n",
      "✅ Written 5766 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_5.csv\n",
      "✅ Written 5847 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_5.csv\n",
      "✅ Written 1905 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_6.csv\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_1.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_2.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_3.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_4.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_5.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "🧹 Cleaning up temp files...\n",
      "✅ Temp cleanup done.\n",
      "🧼 Cleaning and engineering features...\n",
      "✅ Cleaned dataset: 39,994 rows × 17 columns\n",
      "🔐 Connecting to Hopsworks...\n",
      "2025-05-11 18:20:20,566 INFO: Initializing external client\n",
      "2025-05-11 18:20:20,572 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-05-11 18:20:26,343 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1228957\n",
      "🚀 Pushing to Hopsworks Feature Store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JobWarning: All jobs associated to feature group `citi_bike_trips`, version `1` will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Existing feature group deleted.\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1228957/fs/1213523/fg/1454705\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Confluent Kafka package not found. If you want to use Kafka with Hopsworks you can install the corresponding extras via `pip install \"hopsworks[python]\"`. You can also install confluent-kafka directly in your environment with `pip install confluent-kafka`. You will need to restart your kernel if applicable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m df \u001b[38;5;241m=\u001b[39m clean_and_engineer_features(OUTPUT_FILE)\n\u001b[0;32m     28\u001b[0m fs \u001b[38;5;241m=\u001b[39m connect_to_hopsworks()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mpush_to_feature_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mpush_to_feature_store\u001b[1;34m(df, fs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Recreate feature group\u001b[39;00m\n\u001b[0;32m     22\u001b[0m fg \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mcreate_feature_group(\n\u001b[0;32m     23\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mciti_bike_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     event_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarted_at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m \u001b[43mfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwait_for_job\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Feature group created and data inserted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\bike_Taxi\\bike_Taxienv\\lib\\site-packages\\hsfs\\feature_group.py:3012\u001b[0m, in \u001b[0;36mFeatureGroup.insert\u001b[1;34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[0m\n\u001b[0;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3010\u001b[0m     write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline_backfill_every_hr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr\n\u001b[1;32m-> 3012\u001b[0m job, ge_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3015\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3016\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_type()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m   3025\u001b[0m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[0;32m   3026\u001b[0m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[0;32m   3027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_statistics()\n",
      "File \u001b[1;32me:\\bike_Taxi\\bike_Taxienv\\lib\\site-packages\\hsfs\\core\\feature_group_engine.py:224\u001b[0m, in \u001b[0;36mFeatureGroupEngine.insert\u001b[1;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_group_api\u001b[38;5;241m.\u001b[39mdelete_content(feature_group)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 224\u001b[0m     \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbulk_insert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    233\u001b[0m     ge_report,\n\u001b[0;32m    234\u001b[0m )\n",
      "File \u001b[1;32me:\\bike_Taxi\\bike_Taxienv\\lib\\site-packages\\hsfs\\engine\\python.py:815\u001b[0m, in \u001b[0;36mEngine.save_dataframe\u001b[1;34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_dataframe\u001b[39m(\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    802\u001b[0m     feature_group: FeatureGroup,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    809\u001b[0m     validation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    810\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[job\u001b[38;5;241m.\u001b[39mJob]:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28mhasattr\u001b[39m(feature_group, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXTERNAL_FEATURE_GROUP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39monline_enabled\n\u001b[0;32m    814\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m--> 815\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_dataframe_kafka\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    819\u001b[0m         \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_save_dataframe(\n\u001b[0;32m    821\u001b[0m             feature_group,\n\u001b[0;32m    822\u001b[0m             dataframe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    828\u001b[0m             validation_id,\n\u001b[0;32m    829\u001b[0m         )\n",
      "File \u001b[1;32me:\\bike_Taxi\\bike_Taxienv\\lib\\site-packages\\hsfs\\engine\\python.py:1470\u001b[0m, in \u001b[0;36mEngine._write_dataframe_kafka\u001b[1;34m(self, feature_group, dataframe, offline_write_options)\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_write_dataframe_kafka\u001b[39m(\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1465\u001b[0m     feature_group: Union[FeatureGroup, ExternalFeatureGroup],\n\u001b[0;32m   1466\u001b[0m     dataframe: Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pl\u001b[38;5;241m.\u001b[39mDataFrame],\n\u001b[0;32m   1467\u001b[0m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   1468\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[job\u001b[38;5;241m.\u001b[39mJob]:\n\u001b[0;32m   1469\u001b[0m     initial_check_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1470\u001b[0m     producer, headers, feature_writers, writer \u001b[38;5;241m=\u001b[39m \u001b[43mkafka_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_kafka_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert:\n\u001b[0;32m   1477\u001b[0m         \u001b[38;5;66;03m# set initial_check_point to the current offset\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m         initial_check_point \u001b[38;5;241m=\u001b[39m kafka_engine\u001b[38;5;241m.\u001b[39mkafka_get_offsets(\n\u001b[0;32m   1479\u001b[0m             topic_name\u001b[38;5;241m=\u001b[39mfeature_group\u001b[38;5;241m.\u001b[39m_online_topic_name,\n\u001b[0;32m   1480\u001b[0m             feature_store_id\u001b[38;5;241m=\u001b[39mfeature_group\u001b[38;5;241m.\u001b[39mfeature_store_id,\n\u001b[0;32m   1481\u001b[0m             offline_write_options\u001b[38;5;241m=\u001b[39moffline_write_options,\n\u001b[0;32m   1482\u001b[0m             high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1483\u001b[0m         )\n",
      "File \u001b[1;32me:\\bike_Taxi\\bike_Taxienv\\lib\\site-packages\\hsfs\\core\\kafka_engine.py:86\u001b[0m, in \u001b[0;36minit_kafka_resources\u001b[1;34m(feature_group, offline_write_options, num_entries)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert \u001b[38;5;129;01mand\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer,\n\u001b[0;32m     82\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_kafka_headers,\n\u001b[0;32m     83\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_feature_writers,\n\u001b[0;32m     84\u001b[0m         feature_group\u001b[38;5;241m.\u001b[39m_writer,\n\u001b[0;32m     85\u001b[0m     )\n\u001b[1;32m---> 86\u001b[0m producer, headers, feature_writers, writer \u001b[38;5;241m=\u001b[39m \u001b[43m_init_kafka_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_entries\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_group\u001b[38;5;241m.\u001b[39m_multi_part_insert:\n\u001b[0;32m     90\u001b[0m     feature_group\u001b[38;5;241m.\u001b[39m_kafka_producer \u001b[38;5;241m=\u001b[39m producer\n",
      "File \u001b[1;32me:\\bike_Taxi\\bike_Taxienv\\lib\\site-packages\\hsfs\\core\\kafka_engine.py:105\u001b[0m, in \u001b[0;36m_init_kafka_resources\u001b[1;34m(feature_group, offline_write_options, num_entries)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_kafka_resources\u001b[39m(\n\u001b[0;32m     98\u001b[0m     feature_group: Union[FeatureGroup, ExternalFeatureGroup],\n\u001b[0;32m     99\u001b[0m     offline_write_options: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m ]:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# setup kafka producer\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     producer \u001b[38;5;241m=\u001b[39m \u001b[43minit_kafka_producer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# setup headers\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     headers \u001b[38;5;241m=\u001b[39m get_headers(feature_group, num_entries)\n",
      "File \u001b[1;32me:\\bike_Taxi\\bike_Taxienv\\lib\\site-packages\\hopsworks_common\\decorators.py:137\u001b[0m, in \u001b[0;36muses_confluent_kafka.<locals>.g\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mg\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m HAS_CONFLUENT_KAFKA:\n\u001b[1;32m--> 137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(confluent_kafka_not_installed_message)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Confluent Kafka package not found. If you want to use Kafka with Hopsworks you can install the corresponding extras via `pip install \"hopsworks[python]\"`. You can also install confluent-kafka directly in your environment with `pip install confluent-kafka`. You will need to restart your kernel if applicable."
     ]
    }
   ],
   "source": [
    "# RUN THIS TO EXECUTE THE FULL WORKFLOW\n",
    "if os.path.exists(TEMP_FOLDER):\n",
    "    shutil.rmtree(TEMP_FOLDER)\n",
    "os.makedirs(TEMP_FOLDER, exist_ok=True)\n",
    "\n",
    "print(\"🚀 Starting download + extraction...\")\n",
    "for ym in months:\n",
    "    zip_mem = download_zip_to_memory(ym)\n",
    "    if zip_mem:\n",
    "        extract_all_csvs(zip_mem, TEMP_FOLDER)\n",
    "\n",
    "all_csvs = flatten_csvs_folder(TEMP_FOLDER)\n",
    "print(\"🔍 Counting top 3 stations...\")\n",
    "top3 = get_top3_station_names(all_csvs)\n",
    "print(f\"🏆 Top 3 Stations: {top3}\")\n",
    "\n",
    "print(\"📤 Writing filtered data...\")\n",
    "write_top3_data(all_csvs, top3)\n",
    "\n",
    "if not os.path.exists(OUTPUT_FILE) or os.path.getsize(OUTPUT_FILE) == 0:\n",
    "    raise RuntimeError(\"❌ No data written. Check filters or input data.\")\n",
    "\n",
    "print(\"🧹 Cleaning up temp files...\")\n",
    "shutil.rmtree(TEMP_FOLDER)\n",
    "print(\"✅ Temp cleanup done.\")\n",
    "\n",
    "df = clean_and_engineer_features(OUTPUT_FILE)\n",
    "fs = connect_to_hopsworks()\n",
    "push_to_feature_store(df, fs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
