{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d0853d",
   "metadata": {},
   "source": [
    "# Citi Bike Trip Data Pipeline 🚲\n",
    "End-to-end data pipeline for:\n",
    "- Downloading 12 months of Citi Bike data\n",
    "- Extracting and filtering for Top 3 stations\n",
    "- Cleaning and engineering features\n",
    "- Uploading to Hopsworks Feature Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47958175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Hopsworks\n",
    "import hopsworks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed855588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable Constants\n",
    "TEMP_FOLDER = \"tripdata_temp\"\n",
    "OUTPUT_FILE = \"top3_stations_output.csv\"\n",
    "CHUNK_SIZE = 500_000\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"ride_id\", \"rideable_type\", \"started_at\", \"ended_at\",\n",
    "    \"start_station_name\", \"start_station_id\",\n",
    "    \"end_station_name\", \"end_station_id\",\n",
    "    \"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\",\n",
    "    \"member_casual\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526244f",
   "metadata": {},
   "source": [
    "## ⏳ Get Last 12 Months in US/Eastern Timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088c3fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['202504',\n",
       " '202503',\n",
       " '202502',\n",
       " '202501',\n",
       " '202412',\n",
       " '202411',\n",
       " '202410',\n",
       " '202409',\n",
       " '202408',\n",
       " '202407',\n",
       " '202406',\n",
       " '202405']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_last_12_months_est():\n",
    "    eastern = pytz.timezone(\"US/Eastern\")\n",
    "    now_est = datetime.now(eastern)\n",
    "    return [(now_est - relativedelta(months=i + 1)).strftime('%Y%m') for i in range(12)]\n",
    "\n",
    "months = get_last_12_months_est()\n",
    "months\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4e3eb",
   "metadata": {},
   "source": [
    "## 🌐 Download Citi Bike ZIPs\n",
    "Tries both `.zip` and `.csv.zip` formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743f869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip_to_memory(ym):\n",
    "    base_url = \"https://s3.amazonaws.com/tripdata/\"\n",
    "    filenames = [f\"{ym}-citibike-tripdata.zip\", f\"{ym}-citibike-tripdata.csv.zip\"]\n",
    "    for fname in filenames:\n",
    "        url = base_url + fname\n",
    "        try:\n",
    "            print(f\"🌐 Trying: {url}\")\n",
    "            r = requests.get(url, timeout=20)\n",
    "            if r.status_code == 200:\n",
    "                print(f\"✅ Downloaded: {fname}\")\n",
    "                return BytesIO(r.content)\n",
    "            else:\n",
    "                print(f\"❌ Not found: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error downloading {url}: {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6f0a7",
   "metadata": {},
   "source": [
    "## 🗂️ Extract CSVs (including nested ZIPs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8400a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_csvs(zip_bytes_io, extract_to):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_bytes_io) as zf:\n",
    "            for member in zf.namelist():\n",
    "                if member.endswith('.zip'):\n",
    "                    nested_zip_data = zf.read(member)\n",
    "                    with zipfile.ZipFile(BytesIO(nested_zip_data)) as nested_zf:\n",
    "                        for nested_member in nested_zf.namelist():\n",
    "                            if nested_member.endswith('.csv'):\n",
    "                                print(f\"📦 Extracting nested CSV: {nested_member}\")\n",
    "                                nested_zf.extract(nested_member, extract_to)\n",
    "                elif member.endswith('.csv'):\n",
    "                    print(f\"📁 Extracting CSV: {member}\")\n",
    "                    zf.extract(member, extract_to)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error extracting zip: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c17c6d",
   "metadata": {},
   "source": [
    "## 📁 Flatten CSV Paths from Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b026122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_csvs_folder(root_folder):\n",
    "    flat_files = []\n",
    "    for root, _, files in os.walk(root_folder):\n",
    "        for fname in files:\n",
    "            if fname.endswith(\".csv\"):\n",
    "                full_path = os.path.join(root, fname)\n",
    "                flat_files.append(full_path)\n",
    "    return flat_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e75e6a",
   "metadata": {},
   "source": [
    "## 📊 Identify Top 3 Most Frequent Start Stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f929007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top3_station_names(filepaths):\n",
    "    freq = Counter()\n",
    "    for path in filepaths:\n",
    "        try:\n",
    "            for chunk in pd.read_csv(path, usecols=[\"start_station_name\"], dtype={\"start_station_name\": str}, chunksize=CHUNK_SIZE):\n",
    "                chunk = chunk.dropna(subset=[\"start_station_name\"])\n",
    "                freq.update(chunk[\"start_station_name\"])\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {path}: {e}\")\n",
    "    top3 = [name for name, _ in freq.most_common(3)]\n",
    "    return top3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241077aa",
   "metadata": {},
   "source": [
    "## 📝 Write Filtered Rows (Only Top 3 Stations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e0107bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_top3_data(filepaths, top3, output=OUTPUT_FILE):\n",
    "    first_write = True\n",
    "    for path in filepaths:\n",
    "        try:\n",
    "            for chunk in pd.read_csv(path, usecols=TARGET_COLS, chunksize=CHUNK_SIZE, low_memory=False):\n",
    "                chunk = chunk.dropna(subset=[\"start_station_name\"])\n",
    "                filtered = chunk[chunk[\"start_station_name\"].isin(top3)]\n",
    "                if not filtered.empty:\n",
    "                    filtered.to_csv(output, index=False, mode='a' if not first_write else 'w', header=first_write)\n",
    "                    print(f\"✅ Written {len(filtered)} rows from {path}\")\n",
    "                    first_write = False\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02463a77",
   "metadata": {},
   "source": [
    "## 🧼 Clean Data and Engineer Features\n",
    "- Handle nulls\n",
    "- Parse timestamps\n",
    "- Add ride_duration, day_of_week, hour_of_day, month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189947d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_engineer_features(file_path):\n",
    "    print(\"🧼 Cleaning and engineering features...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    df = df.dropna(subset=['started_at', 'ended_at'])\n",
    "\n",
    "    critical_cols = ['ride_id', 'rideable_type', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'member_casual']\n",
    "    df = df.dropna(subset=critical_cols)\n",
    "\n",
    "    df['start_station_name'] = df['start_station_name'].fillna('Unknown')\n",
    "    df['end_station_name'] = df['end_station_name'].fillna('Unknown')\n",
    "    df['start_station_id'] = df['start_station_id'].fillna('-1').astype(str)\n",
    "    df['end_station_id'] = df['end_station_id'].fillna('-1').astype(str)\n",
    "\n",
    "    df['rideable_type'] = df['rideable_type'].astype('category')\n",
    "    df['member_casual'] = df['member_casual'].astype('category')\n",
    "\n",
    "    df['ride_duration_mins'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "    df = df[df['ride_duration_mins'] > 0]\n",
    "\n",
    "    df['day_of_week'] = df['started_at'].dt.day_name()\n",
    "    df['hour_of_day'] = df['started_at'].dt.hour.astype('int32')\n",
    "    df['month'] = df['started_at'].dt.month.astype('int32')\n",
    "\n",
    "    print(f\"✅ Cleaned dataset: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ef209",
   "metadata": {},
   "source": [
    "## 🔐 Connect and Push to Hopsworks Feature Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8027cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_hopsworks():\n",
    "    print(\"🔐 Connecting to Hopsworks...\")\n",
    "    load_dotenv()\n",
    "    project = hopsworks.login(\n",
    "        project=os.getenv(\"HOPSWORKS_PROJECT\"),\n",
    "        api_key_value=os.getenv(\"HOPSWORKS_API_KEY\")\n",
    "    )\n",
    "    return project.get_feature_store()\n",
    "\n",
    "def push_to_feature_store(df, fs):\n",
    "    print(\"🚀 Pushing to Hopsworks Feature Store...\")\n",
    "\n",
    "    try:\n",
    "        # Delete the entire feature group if it exists\n",
    "        fg = fs.get_feature_group(\"citi_bike_trips\", version=1)\n",
    "        fg.delete()\n",
    "        print(\"🗑️ Existing feature group deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ℹ️ Feature group does not exist or could not be deleted: {e}\")\n",
    "\n",
    "    # Recreate feature group\n",
    "    fg = fs.create_feature_group(\n",
    "        name=\"citi_bike_trips\",\n",
    "        version=1,\n",
    "        description=\"Citi Bike data from top 3 stations in last 12 months\",\n",
    "        primary_key=[\"ride_id\"],\n",
    "        event_time=\"started_at\"\n",
    "    )\n",
    "\n",
    "    fg.insert(df, write_options={\"wait_for_job\": True})\n",
    "    print(\"✅ Feature group created and data inserted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cfd2d",
   "metadata": {},
   "source": [
    "## ⚙️ Run the Full ETL Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9cf5ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting download + extraction...\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202504-citibike-tripdata.zip\n",
      "✅ Downloaded: 202504-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202504-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202504-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202504-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202504-citibike-tripdata_4.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202503-citibike-tripdata.zip\n",
      "❌ Not found: https://s3.amazonaws.com/tripdata/202503-citibike-tripdata.zip\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202503-citibike-tripdata.csv.zip\n",
      "✅ Downloaded: 202503-citibike-tripdata.csv.zip\n",
      "📁 Extracting CSV: 202503-citibike-tripdata.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202502-citibike-tripdata.zip\n",
      "✅ Downloaded: 202502-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202502-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202502-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202502-citibike-tripdata_1.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202501-citibike-tripdata.zip\n",
      "✅ Downloaded: 202501-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202501-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202501-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202501-citibike-tripdata_2.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202412-citibike-tripdata.zip\n",
      "✅ Downloaded: 202412-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202412-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202412-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202412-citibike-tripdata_2.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202411-citibike-tripdata.zip\n",
      "✅ Downloaded: 202411-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202411-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202411-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202411-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202411-citibike-tripdata_4.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202410-citibike-tripdata.zip\n",
      "✅ Downloaded: 202410-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_6.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_5.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202410-citibike-tripdata/202410-citibike-tripdata_3.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202409-citibike-tripdata.zip\n",
      "✅ Downloaded: 202409-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202409-citibike-tripdata_5.csv\n",
      "📁 Extracting CSV: __MACOSX/._202409-citibike-tripdata_5.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202408-citibike-tripdata.zip\n",
      "✅ Downloaded: 202408-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_5.csv\n",
      "📁 Extracting CSV: 202408-citibike-tripdata_4.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202407-citibike-tripdata.zip\n",
      "✅ Downloaded: 202407-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202407-citibike-tripdata_5.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202406-citibike-tripdata.zip\n",
      "✅ Downloaded: 202406-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_5.csv\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202406-citibike-tripdata_2.csv\n",
      "🌐 Trying: https://s3.amazonaws.com/tripdata/202405-citibike-tripdata.zip\n",
      "✅ Downloaded: 202405-citibike-tripdata.zip\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_1.csv\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_2.csv\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_3.csv\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_4.csv\n",
      "📁 Extracting CSV: 202405-citibike-tripdata_5.csv\n",
      "🔍 Counting top 3 stations...\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_1.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_2.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_3.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_4.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_5.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "🏆 Top 3 Stations: ['W 21 St & 6 Ave', 'University Pl & E 14 St', 'Lafayette St & E 8 St']\n",
      "📤 Writing filtered data...\n",
      "✅ Written 4269 rows from tripdata_temp\\202405-citibike-tripdata_1.csv\n",
      "✅ Written 4148 rows from tripdata_temp\\202405-citibike-tripdata_1.csv\n",
      "✅ Written 6092 rows from tripdata_temp\\202405-citibike-tripdata_2.csv\n",
      "✅ Written 6035 rows from tripdata_temp\\202405-citibike-tripdata_2.csv\n",
      "✅ Written 3887 rows from tripdata_temp\\202405-citibike-tripdata_3.csv\n",
      "✅ Written 3906 rows from tripdata_temp\\202405-citibike-tripdata_3.csv\n",
      "✅ Written 5128 rows from tripdata_temp\\202405-citibike-tripdata_4.csv\n",
      "✅ Written 4831 rows from tripdata_temp\\202405-citibike-tripdata_4.csv\n",
      "✅ Written 1726 rows from tripdata_temp\\202405-citibike-tripdata_5.csv\n",
      "✅ Written 5017 rows from tripdata_temp\\202406-citibike-tripdata_1.csv\n",
      "✅ Written 4133 rows from tripdata_temp\\202406-citibike-tripdata_1.csv\n",
      "✅ Written 4413 rows from tripdata_temp\\202406-citibike-tripdata_2.csv\n",
      "✅ Written 5777 rows from tripdata_temp\\202406-citibike-tripdata_2.csv\n",
      "✅ Written 5494 rows from tripdata_temp\\202406-citibike-tripdata_3.csv\n",
      "✅ Written 4376 rows from tripdata_temp\\202406-citibike-tripdata_3.csv\n",
      "✅ Written 5421 rows from tripdata_temp\\202406-citibike-tripdata_4.csv\n",
      "✅ Written 4826 rows from tripdata_temp\\202406-citibike-tripdata_4.csv\n",
      "✅ Written 4404 rows from tripdata_temp\\202406-citibike-tripdata_5.csv\n",
      "✅ Written 3119 rows from tripdata_temp\\202406-citibike-tripdata_5.csv\n",
      "✅ Written 4602 rows from tripdata_temp\\202407-citibike-tripdata_1.csv\n",
      "✅ Written 4556 rows from tripdata_temp\\202407-citibike-tripdata_1.csv\n",
      "✅ Written 4116 rows from tripdata_temp\\202407-citibike-tripdata_2.csv\n",
      "✅ Written 5066 rows from tripdata_temp\\202407-citibike-tripdata_2.csv\n",
      "✅ Written 2152 rows from tripdata_temp\\202407-citibike-tripdata_3.csv\n",
      "✅ Written 4749 rows from tripdata_temp\\202407-citibike-tripdata_3.csv\n",
      "✅ Written 4638 rows from tripdata_temp\\202407-citibike-tripdata_4.csv\n",
      "✅ Written 5166 rows from tripdata_temp\\202407-citibike-tripdata_4.csv\n",
      "✅ Written 6684 rows from tripdata_temp\\202407-citibike-tripdata_5.csv\n",
      "✅ Written 2709 rows from tripdata_temp\\202407-citibike-tripdata_5.csv\n",
      "✅ Written 4395 rows from tripdata_temp\\202408-citibike-tripdata_1.csv\n",
      "✅ Written 5096 rows from tripdata_temp\\202408-citibike-tripdata_1.csv\n",
      "✅ Written 5047 rows from tripdata_temp\\202408-citibike-tripdata_2.csv\n",
      "✅ Written 5211 rows from tripdata_temp\\202408-citibike-tripdata_2.csv\n",
      "✅ Written 5087 rows from tripdata_temp\\202408-citibike-tripdata_3.csv\n",
      "✅ Written 4756 rows from tripdata_temp\\202408-citibike-tripdata_3.csv\n",
      "✅ Written 4187 rows from tripdata_temp\\202408-citibike-tripdata_4.csv\n",
      "✅ Written 5129 rows from tripdata_temp\\202408-citibike-tripdata_4.csv\n",
      "✅ Written 4760 rows from tripdata_temp\\202408-citibike-tripdata_5.csv\n",
      "✅ Written 3318 rows from tripdata_temp\\202409-citibike-tripdata_1.csv\n",
      "✅ Written 4382 rows from tripdata_temp\\202409-citibike-tripdata_1.csv\n",
      "✅ Written 3458 rows from tripdata_temp\\202409-citibike-tripdata_2.csv\n",
      "✅ Written 2715 rows from tripdata_temp\\202409-citibike-tripdata_2.csv\n",
      "✅ Written 4848 rows from tripdata_temp\\202409-citibike-tripdata_3.csv\n",
      "✅ Written 4514 rows from tripdata_temp\\202409-citibike-tripdata_3.csv\n",
      "✅ Written 2843 rows from tripdata_temp\\202409-citibike-tripdata_4.csv\n",
      "✅ Written 6104 rows from tripdata_temp\\202409-citibike-tripdata_4.csv\n",
      "✅ Written 5337 rows from tripdata_temp\\202409-citibike-tripdata_5.csv\n",
      "✅ Written 7643 rows from tripdata_temp\\202409-citibike-tripdata_5.csv\n",
      "✅ Written 4622 rows from tripdata_temp\\202411-citibike-tripdata_1.csv\n",
      "✅ Written 4416 rows from tripdata_temp\\202411-citibike-tripdata_1.csv\n",
      "✅ Written 5510 rows from tripdata_temp\\202411-citibike-tripdata_2.csv\n",
      "✅ Written 6165 rows from tripdata_temp\\202411-citibike-tripdata_2.csv\n",
      "✅ Written 5355 rows from tripdata_temp\\202411-citibike-tripdata_3.csv\n",
      "✅ Written 4250 rows from tripdata_temp\\202411-citibike-tripdata_3.csv\n",
      "✅ Written 5364 rows from tripdata_temp\\202411-citibike-tripdata_4.csv\n",
      "✅ Written 2489 rows from tripdata_temp\\202411-citibike-tripdata_4.csv\n",
      "✅ Written 4474 rows from tripdata_temp\\202412-citibike-tripdata_1.csv\n",
      "✅ Written 5606 rows from tripdata_temp\\202412-citibike-tripdata_1.csv\n",
      "✅ Written 7089 rows from tripdata_temp\\202412-citibike-tripdata_2.csv\n",
      "✅ Written 3864 rows from tripdata_temp\\202412-citibike-tripdata_2.csv\n",
      "✅ Written 3053 rows from tripdata_temp\\202412-citibike-tripdata_3.csv\n",
      "✅ Written 4981 rows from tripdata_temp\\202501-citibike-tripdata_1.csv\n",
      "✅ Written 4953 rows from tripdata_temp\\202501-citibike-tripdata_1.csv\n",
      "✅ Written 6306 rows from tripdata_temp\\202501-citibike-tripdata_2.csv\n",
      "✅ Written 5259 rows from tripdata_temp\\202501-citibike-tripdata_2.csv\n",
      "✅ Written 1121 rows from tripdata_temp\\202501-citibike-tripdata_3.csv\n",
      "✅ Written 6623 rows from tripdata_temp\\202502-citibike-tripdata_1.csv\n",
      "✅ Written 4569 rows from tripdata_temp\\202502-citibike-tripdata_1.csv\n",
      "✅ Written 5672 rows from tripdata_temp\\202502-citibike-tripdata_2.csv\n",
      "✅ Written 4767 rows from tripdata_temp\\202502-citibike-tripdata_2.csv\n",
      "✅ Written 340 rows from tripdata_temp\\202502-citibike-tripdata_3.csv\n",
      "✅ Written 5138 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 5613 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 3726 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 4735 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 5927 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 4848 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 1620 rows from tripdata_temp\\202503-citibike-tripdata.csv\n",
      "✅ Written 4910 rows from tripdata_temp\\202504-citibike-tripdata_1.csv\n",
      "✅ Written 4890 rows from tripdata_temp\\202504-citibike-tripdata_1.csv\n",
      "✅ Written 4610 rows from tripdata_temp\\202504-citibike-tripdata_2.csv\n",
      "✅ Written 3922 rows from tripdata_temp\\202504-citibike-tripdata_2.csv\n",
      "✅ Written 6138 rows from tripdata_temp\\202504-citibike-tripdata_3.csv\n",
      "✅ Written 4733 rows from tripdata_temp\\202504-citibike-tripdata_3.csv\n",
      "✅ Written 5533 rows from tripdata_temp\\202504-citibike-tripdata_4.csv\n",
      "✅ Written 2275 rows from tripdata_temp\\202504-citibike-tripdata_4.csv\n",
      "✅ Written 4605 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_1.csv\n",
      "✅ Written 4566 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_1.csv\n",
      "✅ Written 5732 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_2.csv\n",
      "✅ Written 5151 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_2.csv\n",
      "✅ Written 3776 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_3.csv\n",
      "✅ Written 4481 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_3.csv\n",
      "✅ Written 5387 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_4.csv\n",
      "✅ Written 5707 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_4.csv\n",
      "✅ Written 5766 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_5.csv\n",
      "✅ Written 5847 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_5.csv\n",
      "✅ Written 1905 rows from tripdata_temp\\202410-citibike-tripdata\\202410-citibike-tripdata_6.csv\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_1.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_2.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_3.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_4.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "⚠️ Skipping tripdata_temp\\__MACOSX\\._202409-citibike-tripdata_5.csv: 'utf-8' codec can't decode byte 0xb0 in position 45: invalid start byte\n",
      "🧹 Cleaning up temp files...\n",
      "✅ Temp cleanup done.\n",
      "🧼 Cleaning and engineering features...\n",
      "✅ Cleaned dataset: 39,994 rows × 17 columns\n",
      "🔐 Connecting to Hopsworks...\n",
      "2025-05-12 06:43:07,219 INFO: Initializing external client\n",
      "2025-05-12 06:43:07,226 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-05-12 06:43:10,546 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1228957\n",
      "🚀 Pushing to Hopsworks Feature Store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JobWarning: All jobs associated to feature group `citi_bike_trips`, version `1` will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Existing feature group deleted.\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1228957/fs/1213523/fg/1454726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 39994/39994 | Elapsed Time: 00:15 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: citi_bike_trips_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1228957/jobs/named/citi_bike_trips_1_offline_fg_materialization/executions\n",
      "2025-05-12 06:44:03,246 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-05-12 06:44:06,341 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2025-05-12 06:46:22,575 INFO: Waiting for execution to finish. Current state: SUCCEEDING. Final status: UNDEFINED\n",
      "2025-05-12 06:46:31,843 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-05-12 06:46:31,942 INFO: Waiting for log aggregation to finish.\n",
      "2025-05-12 06:46:40,333 INFO: Execution finished successfully.\n",
      "✅ Feature group created and data inserted.\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS TO EXECUTE THE FULL WORKFLOW\n",
    "if os.path.exists(TEMP_FOLDER):\n",
    "    shutil.rmtree(TEMP_FOLDER)\n",
    "os.makedirs(TEMP_FOLDER, exist_ok=True)\n",
    "\n",
    "print(\"🚀 Starting download + extraction...\")\n",
    "for ym in months:\n",
    "    zip_mem = download_zip_to_memory(ym)\n",
    "    if zip_mem:\n",
    "        extract_all_csvs(zip_mem, TEMP_FOLDER)\n",
    "\n",
    "all_csvs = flatten_csvs_folder(TEMP_FOLDER)\n",
    "print(\"🔍 Counting top 3 stations...\")\n",
    "top3 = get_top3_station_names(all_csvs)\n",
    "print(f\"🏆 Top 3 Stations: {top3}\")\n",
    "\n",
    "print(\"📤 Writing filtered data...\")\n",
    "write_top3_data(all_csvs, top3)\n",
    "\n",
    "if not os.path.exists(OUTPUT_FILE) or os.path.getsize(OUTPUT_FILE) == 0:\n",
    "    raise RuntimeError(\"❌ No data written. Check filters or input data.\")\n",
    "\n",
    "print(\"🧹 Cleaning up temp files...\")\n",
    "shutil.rmtree(TEMP_FOLDER)\n",
    "print(\"✅ Temp cleanup done.\")\n",
    "\n",
    "df = clean_and_engineer_features(OUTPUT_FILE)\n",
    "fs = connect_to_hopsworks()\n",
    "push_to_feature_store(df, fs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
